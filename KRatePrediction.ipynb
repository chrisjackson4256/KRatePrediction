{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting MLB Strikeout Rates from Velocity and Movement\n",
    "\n",
    "Using data from 2012-2016, we first determine a classification system for different pitches using a clustering algorithm.  Motivation for this work was provided by http://www.sloansportsconference.com/wp-content/uploads/2019/02/Predicting-Major-League-Baseball-Strikeout-Rates-Update.pdf.\n",
    "\n",
    "These features are then used to construct a model to predict strikeout rates using several regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from pybaseball import statcast, pitching_stats, pitching_stats_range\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the data\n",
    "\n",
    "The raw data is scraped from statcast and/or pitch fx using the pybaseball module.  We remove \"short relievers\" by keeping only pitchers that have thrown at least 1000 pitches in a season AND that average 10 batters faced per appearance.\n",
    "\n",
    "The data columns used from the pitcher data are:\n",
    "\n",
    "- game_pk: the unique game ID\n",
    "- inning: inning of the pitch\n",
    "- game_year: the year of the data\n",
    "- player_name: the pitcher's name (this is used to join to a separate data pull that is required to extract strikeouts per 9 innings)\n",
    "- batter: used to keep track of batters faced\n",
    "- release_speed: speed of the ball at time of release (feature used for pitch categorization in the clustering algorithm)\n",
    "- pfx_x, pfx_z: movement in the horizontal and vertical directions (features used for pitch categorization in the clustering algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_batters_faced(df, pitcher_id):\n",
    "    \n",
    "    # subset the given pitcher's games\n",
    "    temp_df = df[df['pitcher']==pitcher_id]\n",
    "    \n",
    "    # count number of batters faced in each game\n",
    "    temp_df = temp_df.drop_duplicates(subset=['game_pk', 'inning', 'batter'])\n",
    "    total_batters_faced = len(temp_df)\n",
    "\n",
    "    # count the number of appearances\n",
    "    number_of_games = len(temp_df['game_pk'].unique())\n",
    "    \n",
    "    # return the average number of batters faced per game\n",
    "    return round(total_batters_faced / number_of_games, 1)\n",
    "\n",
    "\n",
    "def get_speed_location_data(start, end):\n",
    "\n",
    "    # get the raw pitch data from statcast\n",
    "    pitch_data = statcast(start_dt=start, end_dt=end)\n",
    "    \n",
    "    # make sure index columns are int\n",
    "    pitch_data['game_pk'] = pitch_data['game_pk'].astype(int)\n",
    "    pitch_data['inning'] = pitch_data['inning'].astype(int)\n",
    "    pitch_data['game_year'] = pitch_data['game_year'].astype(int)\n",
    "    pitch_data['pitcher'] = pitch_data['pitcher'].astype(int)\n",
    "    pitch_data['batter'] = pitch_data['batter'].astype(int)\n",
    "    \n",
    "    # get the data for K rate and to compute the pitcher's strike percentage\n",
    "    strike_data = pitching_stats(start[:4])\n",
    "    strike_data = strike_data[['Season', 'Name', 'K/9', 'Pitches', 'Strikes']]\n",
    "    strike_data.columns = ['game_year', 'player_name', 'K/9', 'Pitches', 'Strikes']\n",
    "    strike_data['strike_pct'] = strike_data['Strikes'] / strike_data['Pitches']\n",
    "    strike_data.drop(['Pitches', 'Strikes'], axis=1, inplace=True)\n",
    "    strike_data['game_year'] = strike_data['game_year'].astype(int)\n",
    "    \n",
    "    # merge the two dataframes together\n",
    "    pitch_data = pd.merge(pitch_data, strike_data, how='inner', on=['game_year', 'player_name'])\n",
    "        \n",
    "    # select the columns that we need\n",
    "    cols_to_keep = ['game_pk', 'inning', 'game_year', 'player_name', 'pitcher', 'batter', 'release_speed', 'pfx_x', 'pfx_z', 'strike_pct', 'K/9']\n",
    "    pitch_data = pitch_data[cols_to_keep]\n",
    "    \n",
    "    # drop any rows that have no game_pk\n",
    "    pitch_data = pitch_data[~pd.isnull(pitch_data['game_pk'])]\n",
    "    \n",
    "    # make an index for observations \n",
    "    pitch_data['obs_index'] = pitch_data['game_year'].astype(str) + \"_\" + pitch_data['pitcher'].astype(str)\n",
    "    \n",
    "    # get a count of pitchers and number of pitches they threw\n",
    "    pitcher_count = dict(Counter(pitch_data['pitcher']))\n",
    "\n",
    "    # list of pitchers with at least 1000 pitches thrown\n",
    "    pitchers_w_1000pitches = [k for k, v in pitcher_count.items() if v >= 1000]\n",
    "    \n",
    "    # subset the dataframe to those pitchers with at least 1000 pitches thrown\n",
    "    pitch_data = pitch_data[pitch_data['pitcher'].isin(pitchers_w_1000pitches)]\n",
    "    \n",
    "    # list of pitchers with at least 1000 pitches thrown AND average number of batters faced per outing greater than 10\n",
    "    pitchers_no_short = [x for x in pitchers_w_1000pitches if avg_batters_faced(pitch_data, x) >= 10.0]\n",
    "\n",
    "    # subset the dataframe to those pitchers with at least 1000 pitches thrown\n",
    "    pitch_data = pitch_data[pitch_data['pitcher'].isin(pitchers_no_short)]\n",
    "        \n",
    "    # keep only columns we need\n",
    "    pitch_data = pitch_data[['obs_index', 'pitcher', 'player_name', 'release_speed', 'pfx_x', 'pfx_z', 'strike_pct', 'K/9']]\n",
    "        \n",
    "    return pitch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the data using pybaseball\n",
    "\n",
    "Fill in the start and end dates of the season of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a large query, it may take a moment to complete\n",
      "Completed sub-query from 2012-03-28 to 2012-04-02\n",
      "Completed sub-query from 2012-04-03 to 2012-04-08\n",
      "Completed sub-query from 2012-04-09 to 2012-04-14\n",
      "Completed sub-query from 2012-04-15 to 2012-04-20\n",
      "Completed sub-query from 2012-04-21 to 2012-04-26\n",
      "Completed sub-query from 2012-04-27 to 2012-05-02\n",
      "Completed sub-query from 2012-05-03 to 2012-05-08\n",
      "Completed sub-query from 2012-05-09 to 2012-05-14\n",
      "Completed sub-query from 2012-05-15 to 2012-05-20\n",
      "Completed sub-query from 2012-05-21 to 2012-05-26\n",
      "Completed sub-query from 2012-05-27 to 2012-06-01\n",
      "Completed sub-query from 2012-06-02 to 2012-06-07\n",
      "Completed sub-query from 2012-06-08 to 2012-06-13\n",
      "Completed sub-query from 2012-06-14 to 2012-06-19\n",
      "Completed sub-query from 2012-06-20 to 2012-06-25\n",
      "Completed sub-query from 2012-06-26 to 2012-07-01\n",
      "Completed sub-query from 2012-07-02 to 2012-07-07\n",
      "Completed sub-query from 2012-07-08 to 2012-07-13\n",
      "Completed sub-query from 2012-07-14 to 2012-07-19\n",
      "Completed sub-query from 2012-07-20 to 2012-07-25\n",
      "Completed sub-query from 2012-07-26 to 2012-07-31\n",
      "Completed sub-query from 2012-08-01 to 2012-08-06\n",
      "Completed sub-query from 2012-08-07 to 2012-08-12\n",
      "Completed sub-query from 2012-08-13 to 2012-08-18\n",
      "Completed sub-query from 2012-08-19 to 2012-08-24\n",
      "Completed sub-query from 2012-08-25 to 2012-08-30\n",
      "Completed sub-query from 2012-08-31 to 2012-09-05\n",
      "Completed sub-query from 2012-09-06 to 2012-09-11\n",
      "Completed sub-query from 2012-09-12 to 2012-09-17\n",
      "Completed sub-query from 2012-09-18 to 2012-09-23\n",
      "Completed sub-query from 2012-09-24 to 2012-09-29\n",
      "Completed sub-query from 2012-09-30 to 2012-10-03\n",
      "This is a large query, it may take a moment to complete\n",
      "Completed sub-query from 2013-03-31 to 2013-04-05\n",
      "Completed sub-query from 2013-04-06 to 2013-04-11\n",
      "Completed sub-query from 2013-04-12 to 2013-04-17\n",
      "Completed sub-query from 2013-04-18 to 2013-04-23\n",
      "Completed sub-query from 2013-04-24 to 2013-04-29\n",
      "Completed sub-query from 2013-04-30 to 2013-05-05\n",
      "Completed sub-query from 2013-05-06 to 2013-05-11\n",
      "Completed sub-query from 2013-05-12 to 2013-05-17\n",
      "Completed sub-query from 2013-05-18 to 2013-05-23\n",
      "Completed sub-query from 2013-05-24 to 2013-05-29\n",
      "Completed sub-query from 2013-05-30 to 2013-06-04\n",
      "Completed sub-query from 2013-06-05 to 2013-06-10\n",
      "Completed sub-query from 2013-06-11 to 2013-06-16\n",
      "Completed sub-query from 2013-06-17 to 2013-06-22\n",
      "Completed sub-query from 2013-06-23 to 2013-06-28\n",
      "Completed sub-query from 2013-06-29 to 2013-07-04\n",
      "Completed sub-query from 2013-07-05 to 2013-07-10\n",
      "Completed sub-query from 2013-07-11 to 2013-07-16\n",
      "Completed sub-query from 2013-07-17 to 2013-07-22\n",
      "Completed sub-query from 2013-07-23 to 2013-07-28\n",
      "Completed sub-query from 2013-07-29 to 2013-08-03\n",
      "Completed sub-query from 2013-08-04 to 2013-08-09\n",
      "Completed sub-query from 2013-08-10 to 2013-08-15\n",
      "Completed sub-query from 2013-08-16 to 2013-08-21\n",
      "Completed sub-query from 2013-08-22 to 2013-08-27\n",
      "Completed sub-query from 2013-08-28 to 2013-09-02\n",
      "Completed sub-query from 2013-09-03 to 2013-09-08\n",
      "Completed sub-query from 2013-09-09 to 2013-09-14\n",
      "Completed sub-query from 2013-09-15 to 2013-09-20\n",
      "Completed sub-query from 2013-09-21 to 2013-09-26\n",
      "Completed sub-query from 2013-09-27 to 2013-09-30\n",
      "This is a large query, it may take a moment to complete\n",
      "Completed sub-query from 2014-03-22 to 2014-03-27\n",
      "Completed sub-query from 2014-03-28 to 2014-04-02\n",
      "Completed sub-query from 2014-04-03 to 2014-04-08\n",
      "Completed sub-query from 2014-04-09 to 2014-04-14\n",
      "Completed sub-query from 2014-04-15 to 2014-04-20\n",
      "Completed sub-query from 2014-04-21 to 2014-04-26\n",
      "Completed sub-query from 2014-04-27 to 2014-05-02\n",
      "Completed sub-query from 2014-05-03 to 2014-05-08\n",
      "Completed sub-query from 2014-05-09 to 2014-05-14\n",
      "Completed sub-query from 2014-05-15 to 2014-05-20\n",
      "Completed sub-query from 2014-05-21 to 2014-05-26\n",
      "Completed sub-query from 2014-05-27 to 2014-06-01\n",
      "Completed sub-query from 2014-06-02 to 2014-06-07\n",
      "Completed sub-query from 2014-06-08 to 2014-06-13\n",
      "Completed sub-query from 2014-06-14 to 2014-06-19\n",
      "Completed sub-query from 2014-06-20 to 2014-06-25\n",
      "Completed sub-query from 2014-06-26 to 2014-07-01\n",
      "Completed sub-query from 2014-07-02 to 2014-07-07\n",
      "Completed sub-query from 2014-07-08 to 2014-07-13\n",
      "Completed sub-query from 2014-07-14 to 2014-07-19\n",
      "Completed sub-query from 2014-07-20 to 2014-07-25\n",
      "Completed sub-query from 2014-07-26 to 2014-07-31\n",
      "Completed sub-query from 2014-08-01 to 2014-08-06\n",
      "Completed sub-query from 2014-08-07 to 2014-08-12\n",
      "Completed sub-query from 2014-08-13 to 2014-08-18\n",
      "Completed sub-query from 2014-08-19 to 2014-08-24\n",
      "Completed sub-query from 2014-08-25 to 2014-08-30\n",
      "Completed sub-query from 2014-08-31 to 2014-09-05\n",
      "Completed sub-query from 2014-09-06 to 2014-09-11\n",
      "Completed sub-query from 2014-09-12 to 2014-09-17\n",
      "Completed sub-query from 2014-09-18 to 2014-09-23\n",
      "Completed sub-query from 2014-09-24 to 2014-09-28\n",
      "This is a large query, it may take a moment to complete\n",
      "Completed sub-query from 2015-04-05 to 2015-04-10\n",
      "Completed sub-query from 2015-04-11 to 2015-04-16\n",
      "Completed sub-query from 2015-04-17 to 2015-04-22\n",
      "Completed sub-query from 2015-04-23 to 2015-04-28\n",
      "Completed sub-query from 2015-04-29 to 2015-05-04\n",
      "Completed sub-query from 2015-05-05 to 2015-05-10\n",
      "Completed sub-query from 2015-05-11 to 2015-05-16\n",
      "Completed sub-query from 2015-05-17 to 2015-05-22\n",
      "Completed sub-query from 2015-05-23 to 2015-05-28\n",
      "Completed sub-query from 2015-05-29 to 2015-06-03\n",
      "Completed sub-query from 2015-06-04 to 2015-06-09\n",
      "Completed sub-query from 2015-06-10 to 2015-06-15\n",
      "Completed sub-query from 2015-06-16 to 2015-06-21\n",
      "Completed sub-query from 2015-06-22 to 2015-06-27\n",
      "Completed sub-query from 2015-06-28 to 2015-07-03\n",
      "Completed sub-query from 2015-07-04 to 2015-07-09\n",
      "Completed sub-query from 2015-07-10 to 2015-07-15\n",
      "Completed sub-query from 2015-07-16 to 2015-07-21\n",
      "Completed sub-query from 2015-07-22 to 2015-07-27\n",
      "Completed sub-query from 2015-07-28 to 2015-08-02\n",
      "Completed sub-query from 2015-08-03 to 2015-08-08\n",
      "Completed sub-query from 2015-08-09 to 2015-08-14\n",
      "Completed sub-query from 2015-08-15 to 2015-08-20\n",
      "Completed sub-query from 2015-08-21 to 2015-08-26\n",
      "Completed sub-query from 2015-08-27 to 2015-09-01\n",
      "Completed sub-query from 2015-09-02 to 2015-09-07\n",
      "Completed sub-query from 2015-09-08 to 2015-09-13\n",
      "Completed sub-query from 2015-09-14 to 2015-09-19\n",
      "Completed sub-query from 2015-09-20 to 2015-09-25\n",
      "Completed sub-query from 2015-09-26 to 2015-10-01\n",
      "Completed sub-query from 2015-10-02 to 2015-10-04\n",
      "This is a large query, it may take a moment to complete\n",
      "Completed sub-query from 2016-04-03 to 2016-04-08\n",
      "Completed sub-query from 2016-04-09 to 2016-04-14\n",
      "Completed sub-query from 2016-04-15 to 2016-04-20\n",
      "Completed sub-query from 2016-04-21 to 2016-04-26\n",
      "Completed sub-query from 2016-04-27 to 2016-05-02\n",
      "Completed sub-query from 2016-05-03 to 2016-05-08\n",
      "Completed sub-query from 2016-05-09 to 2016-05-14\n",
      "Completed sub-query from 2016-05-15 to 2016-05-20\n",
      "Completed sub-query from 2016-05-21 to 2016-05-26\n",
      "Completed sub-query from 2016-05-27 to 2016-06-01\n",
      "Completed sub-query from 2016-06-02 to 2016-06-07\n",
      "Completed sub-query from 2016-06-08 to 2016-06-13\n",
      "Completed sub-query from 2016-06-14 to 2016-06-19\n",
      "Completed sub-query from 2016-06-20 to 2016-06-25\n",
      "Completed sub-query from 2016-06-26 to 2016-07-01\n",
      "Completed sub-query from 2016-07-02 to 2016-07-07\n",
      "Completed sub-query from 2016-07-08 to 2016-07-13\n",
      "Completed sub-query from 2016-07-14 to 2016-07-19\n",
      "Completed sub-query from 2016-07-20 to 2016-07-25\n",
      "Completed sub-query from 2016-07-26 to 2016-07-31\n",
      "Completed sub-query from 2016-08-01 to 2016-08-06\n",
      "Completed sub-query from 2016-08-07 to 2016-08-12\n",
      "Completed sub-query from 2016-08-13 to 2016-08-18\n",
      "Completed sub-query from 2016-08-19 to 2016-08-24\n",
      "Completed sub-query from 2016-08-25 to 2016-08-30\n",
      "Completed sub-query from 2016-08-31 to 2016-09-05\n",
      "Completed sub-query from 2016-09-06 to 2016-09-11\n",
      "Completed sub-query from 2016-09-12 to 2016-09-17\n",
      "Completed sub-query from 2016-09-18 to 2016-09-23\n",
      "Completed sub-query from 2016-09-24 to 2016-09-29\n",
      "Completed sub-query from 2016-09-30 to 2016-10-02\n",
      "Shape of training data: (2108087, 8)\n",
      "Number of unique pitchers in training data: 355\n",
      "Number of unique observations in training data: 884\n"
     ]
    }
   ],
   "source": [
    "train_data_dates = [('2012-03-28', '2012-10-03'),\n",
    "                    ('2013-03-31', '2013-09-30'),\n",
    "                    ('2014-03-22', '2014-09-28'),\n",
    "                    ('2015-04-05', '2015-10-04'),\n",
    "                    ('2016-04-03', '2016-10-02')]\n",
    "\n",
    "train_data_list = []\n",
    "for dates in train_data_dates:\n",
    "    df = get_speed_location_data(start=dates[0], end=dates[1])\n",
    "    train_data_list.append(df)\n",
    "    \n",
    "train_data = pd.concat(train_data_list)\n",
    "print(f\"Shape of training data: {train_data.shape}\")\n",
    "print(f\"Number of unique pitchers in training data: {len(train_data['pitcher'].unique())}\")\n",
    "print(f\"Number of unique observations in training data: {len(train_data['obs_index'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a large query, it may take a moment to complete\n",
      "Completed sub-query from 2017-04-02 to 2017-04-07\n",
      "Completed sub-query from 2017-04-08 to 2017-04-13\n",
      "Completed sub-query from 2017-04-14 to 2017-04-19\n",
      "Completed sub-query from 2017-04-20 to 2017-04-25\n",
      "Completed sub-query from 2017-04-26 to 2017-05-01\n",
      "Completed sub-query from 2017-05-02 to 2017-05-07\n",
      "Completed sub-query from 2017-05-08 to 2017-05-13\n",
      "Completed sub-query from 2017-05-14 to 2017-05-19\n",
      "Completed sub-query from 2017-05-20 to 2017-05-25\n",
      "Completed sub-query from 2017-05-26 to 2017-05-31\n",
      "Completed sub-query from 2017-06-01 to 2017-06-06\n",
      "Completed sub-query from 2017-06-07 to 2017-06-12\n",
      "Completed sub-query from 2017-06-13 to 2017-06-18\n",
      "Completed sub-query from 2017-06-19 to 2017-06-24\n",
      "Completed sub-query from 2017-06-25 to 2017-06-30\n",
      "Completed sub-query from 2017-07-01 to 2017-07-06\n",
      "Completed sub-query from 2017-07-07 to 2017-07-12\n",
      "Completed sub-query from 2017-07-13 to 2017-07-18\n",
      "Completed sub-query from 2017-07-19 to 2017-07-24\n",
      "Completed sub-query from 2017-07-25 to 2017-07-30\n",
      "Completed sub-query from 2017-07-31 to 2017-08-05\n",
      "Completed sub-query from 2017-08-06 to 2017-08-11\n",
      "Completed sub-query from 2017-08-12 to 2017-08-17\n",
      "Completed sub-query from 2017-08-18 to 2017-08-23\n",
      "Completed sub-query from 2017-08-24 to 2017-08-29\n",
      "Completed sub-query from 2017-08-30 to 2017-09-04\n",
      "Completed sub-query from 2017-09-05 to 2017-09-10\n",
      "Completed sub-query from 2017-09-11 to 2017-09-16\n",
      "Completed sub-query from 2017-09-17 to 2017-09-22\n",
      "Completed sub-query from 2017-09-23 to 2017-09-28\n",
      "Completed sub-query from 2017-09-29 to 2017-10-01\n",
      "Shape of test data: (419462, 8)\n",
      "Number of unique pitchers in test data: 190\n",
      "Number of unique observations in test data: 190\n"
     ]
    }
   ],
   "source": [
    "test_data_dates = [('2017-04-02', '2017-10-01')]\n",
    "\n",
    "test_data_list = []\n",
    "for dates in test_data_dates:\n",
    "    df = get_speed_location_data(start=dates[0], end=dates[1])\n",
    "    test_data_list.append(df)\n",
    "    \n",
    "test_data = pd.concat(test_data_list)\n",
    "print(f\"Shape of test data: {test_data.shape}\")\n",
    "print(f\"Number of unique pitchers in test data: {len(test_data['pitcher'].unique())}\")\n",
    "print(f\"Number of unique observations in test data: {len(test_data['obs_index'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the pitch type cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pitches in the clustering data: 2105256\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pfx_x</th>\n",
       "      <th>pfx_z</th>\n",
       "      <th>release_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>0.067617</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>82.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>-0.580900</td>\n",
       "      <td>-0.348967</td>\n",
       "      <td>74.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6837</th>\n",
       "      <td>1.224092</td>\n",
       "      <td>1.223400</td>\n",
       "      <td>82.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>1.392483</td>\n",
       "      <td>1.505767</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6839</th>\n",
       "      <td>1.141983</td>\n",
       "      <td>1.429800</td>\n",
       "      <td>80.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pfx_x     pfx_z  release_speed\n",
       "6835  0.067617  0.019400           82.7\n",
       "6836 -0.580900 -0.348967           74.9\n",
       "6837  1.224092  1.223400           82.6\n",
       "6838  1.392483  1.505767           94.0\n",
       "6839  1.141983  1.429800           80.8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_train_data = train_data[['pfx_x', 'pfx_z', 'release_speed']]\n",
    "cluster_train_data.dropna(inplace=True)\n",
    "\n",
    "print(f\"Number of pitches in the clustering data: {cluster_train_data.shape[0]}\\n\")\n",
    "cluster_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standardize the data: scale the features to be between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pfx_x</th>\n",
       "      <th>pfx_z</th>\n",
       "      <th>release_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.521931</td>\n",
       "      <td>0.444919</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.413785</td>\n",
       "      <td>0.399181</td>\n",
       "      <td>0.574534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.714783</td>\n",
       "      <td>0.594412</td>\n",
       "      <td>0.694099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.742864</td>\n",
       "      <td>0.629471</td>\n",
       "      <td>0.871118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.701091</td>\n",
       "      <td>0.620039</td>\n",
       "      <td>0.666149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pfx_x     pfx_z  release_speed\n",
       "0  0.521931  0.444919       0.695652\n",
       "1  0.413785  0.399181       0.574534\n",
       "2  0.714783  0.594412       0.694099\n",
       "3  0.742864  0.629471       0.871118\n",
       "4  0.701091  0.620039       0.666149"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfx_x_scaler = MinMaxScaler()\n",
    "\n",
    "pfx_x = cluster_train_data['pfx_x'].as_matrix().reshape(-1, 1)\n",
    "pfx_x_scaled = pd.DataFrame(pfx_x_scaler.fit_transform(pfx_x))\n",
    "\n",
    "pfx_z_scaler = MinMaxScaler()\n",
    "pfx_z = cluster_train_data['pfx_z'].as_matrix().reshape(-1, 1)\n",
    "pfx_z_scaled = pd.DataFrame(pfx_z_scaler.fit_transform(pfx_z))\n",
    "\n",
    "release_speed_scaler = MinMaxScaler()\n",
    "release_speed = cluster_train_data['release_speed'].as_matrix().reshape(-1, 1)\n",
    "release_speed_scaled = pd.DataFrame(release_speed_scaler.fit_transform(release_speed))\n",
    "\n",
    "cluster_train_data_scaled = pd.merge(pfx_x_scaled, pfx_z_scaled, left_index=True, right_index=True)\n",
    "cluster_train_data_scaled = pd.merge(cluster_train_data_scaled, release_speed_scaled, left_index=True, right_index=True)\n",
    "\n",
    "cluster_train_data_scaled.columns = ['pfx_x', 'pfx_z', 'release_speed']\n",
    "cluster_train_data_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b706d7604ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_train_data_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/sports/lib/python3.6/site-packages/sklearn/cluster/hierarchical.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    829\u001b[0m             memory.cache(tree_builder)(X, connectivity,\n\u001b[1;32m    830\u001b[0m                                        \u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                                        **kwargs)\n\u001b[0m\u001b[1;32m    832\u001b[0m         \u001b[0;31m# Cut the tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompute_full_tree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/sports/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/sports/lib/python3.6/site-packages/sklearn/cluster/hierarchical.py\u001b[0m in \u001b[0;36mward_tree\u001b[0;34m(X, connectivity, n_clusters, return_distance)\u001b[0m\n\u001b[1;32m    232\u001b[0m                           stacklevel=2)\n\u001b[1;32m    233\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequirements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mchildren_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/sports/lib/python3.6/site-packages/scipy/cluster/hierarchy.py\u001b[0m in \u001b[0;36mward\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \"\"\"\n\u001b[0;32m--> 878\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/sports/lib/python3.6/site-packages/scipy/cluster/hierarchy.py\u001b[0m in \u001b[0;36mlinkage\u001b[0;34m(y, method, metric, optimal_ordering)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                          \u001b[0;34m'matrix looks suspiciously like an uncondensed '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                          'distance matrix')\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`y` must be 1 or 2 dimensional.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/sports/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mpdist\u001b[0;34m(X, metric, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m         \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1986\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=9).fit(cluster_train_data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "db = DBSCAN(algorithm='ball_tree', n_jobs=-1).fit(cluster_train_data_scaled)\n",
    "labels = db.labels_\n",
    "k = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "\n",
    "print(f\"Optimal number of clusters: {k}\")\n",
    "print(f\"Number of noise poitns: {n_noise}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dbscan_dist = dict(Counter(pitch_cluster_list))\n",
    "dbscan_x = list(dbscan_dist.keys())\n",
    "dbscan_y = list(dbscan_dist.values())\n",
    "plt.bar(dbscan_x, dbscan_y, alpha=0.5)\n",
    "plt.bar([3,4,5,6,7,8,9], [105,287,235,114,79,42,32], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dbscan_dist = dict(Counter(pitch_cluster_list))\n",
    "dbscan_x = list(dbscan_dist.keys())\n",
    "dbscan_y = list(dbscan_dist.values())\n",
    "plt.bar(dbscan_x, dbscan_y, alpha=0.5)\n",
    "plt.bar([3,4,5,6,7,8,9], [105,287,235,114,79,42,32], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means with Elbow method and Silhouette averages to find optimal k"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# range of number of clusters\n",
    "num_clusters = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]  #range(2, 10)\n",
    "\n",
    "# initiate the models\n",
    "kmeans = []\n",
    "for i in num_clusters:\n",
    "    print(f\"initializing a model for {i} clusters\")\n",
    "    kmeans.append(KMeans(n_clusters=i, n_jobs=-1))\n",
    "\n",
    "# compute the scores for each fit\n",
    "score = []\n",
    "for i, k in enumerate(num_clusters):\n",
    "    print(f\"fitting the data to {k} clusters\")\n",
    "    score.append(kmeans[i].fit(cluster_train_data_scaled).score(cluster_train_data_scaled))\n",
    "\n",
    "# cluster labels\n",
    "cluster_labels = []\n",
    "for i, k in enumerate(num_clusters):\n",
    "    print(f\"labeling the data for {k} clusters\")\n",
    "    cluster_labels.append(kmeans[i].fit_predict(cluster_train_data_scaled))\n",
    "\n",
    "# silhouette avgs\n",
    "silhouette_avgs = []\n",
    "for i, k in enumerate(num_clusters):\n",
    "    print(f\"computing silhouette avgs for {k} clusters\")\n",
    "    silhouette_avgs.append(silhouette_score(cluster_train_data_scaled, cluster_labels[i]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(num_clusters, score)\n",
    "plt.title(\"Elbow Curve\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"K-Means Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(num_clusters, silhouette_avgs)\n",
    "plt.title(\"Silhouette Averages\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silouette Average\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means with Gap statistic to find optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimalK(data, nrefs=3, maxClusters=15):\n",
    "    \"\"\"\n",
    "    Calculates KMeans optimal K using Gap Statistic from Tibshirani, Walther, Hastie\n",
    "    Params:\n",
    "        data: ndarry of shape (n_samples, n_features)\n",
    "        nrefs: number of sample reference datasets to create\n",
    "        maxClusters: Maximum number of clusters to test for\n",
    "    Returns: (gaps, optimalK)\n",
    "    \"\"\"\n",
    "    gaps = np.zeros((len(range(1, maxClusters)),))\n",
    "    resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})\n",
    "    for gap_index, k in enumerate(range(1, maxClusters)):\n",
    "\n",
    "        # Holder for reference dispersion results\n",
    "        refDisps = np.zeros(nrefs)\n",
    "\n",
    "        # For n references, generate random sample and perform kmeans getting resulting dispersion of each loop\n",
    "        for i in range(nrefs):\n",
    "            \n",
    "            # Create new random reference set\n",
    "            randomReference = np.random.random_sample(size=data.shape)\n",
    "            \n",
    "            # Fit to it\n",
    "            km = KMeans(k, n_jobs=-1)\n",
    "            km.fit(randomReference)\n",
    "            \n",
    "            refDisp = km.inertia_\n",
    "            refDisps[i] = refDisp\n",
    "\n",
    "        # Fit cluster to original data and create dispersion\n",
    "        km = KMeans(k)\n",
    "        km.fit(data)\n",
    "        \n",
    "        origDisp = km.inertia_\n",
    "\n",
    "        # Calculate gap statistic\n",
    "        gap = np.log(np.mean(refDisps)) - np.log(origDisp)\n",
    "\n",
    "        # Assign this loop's gap statistic to gaps\n",
    "        gaps[gap_index] = gap\n",
    "        \n",
    "        resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)\n",
    "\n",
    "    return (gaps.argmax() + 1, resultsdf)  # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal\n",
    "\n",
    "k, _ = optimalK(cluster_train_data_scaled)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform k-means clustering (with number of clusters determined from elbow method)\n",
    "kmeans = KMeans(n_clusters=3, n_jobs=-1).fit(cluster_train_data_scaled)\n",
    "\n",
    "print(kmeans.score(cluster_train_data_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorize pitches using k-means clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_pitches(df):\n",
    "\n",
    "    df = df[['obs_index', 'player_name', 'pfx_x', 'pfx_z', 'release_speed']]\n",
    "\n",
    "    df['pfx_x_scaled'] = pfx_x_scaler.transform(df['pfx_x'].as_matrix().reshape(-1,1))\n",
    "    df['pfx_z_scaled'] = pfx_z_scaler.transform(df['pfx_z'].as_matrix().reshape(-1,1))\n",
    "    df['release_speed_scaled'] = release_speed_scaler.transform(df['release_speed'].as_matrix().reshape(-1,1))\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df[['obs_index', 'player_name', 'pfx_x_scaled', 'pfx_z_scaled', 'release_speed_scaled']]\n",
    "\n",
    "    df['cluster_number'] = ac.predict(df[['pfx_x_scaled', 'pfx_z_scaled', 'release_speed_scaled']]) \n",
    "    \n",
    "    return df\n",
    "\n",
    "train_data_categorized = categorize_pitches(train_data)\n",
    "\n",
    "test_data_categorized = categorize_pitches(test_data)\n",
    "\n",
    "test_data_categorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### turn counts of cluster types into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_counts_to_features(df, thresh):\n",
    "    \n",
    "    obs_list = list(df['obs_index'].unique())\n",
    "\n",
    "    pitch_counts_df = pd.DataFrame(columns=[0,1,2])\n",
    "\n",
    "    for i, obs in enumerate(obs_list):\n",
    "\n",
    "        temp_df = df[df['obs_index']==obs]\n",
    "\n",
    "        pitch_df = pd.DataFrame(dict(Counter(temp_df['cluster_number'])), index=[obs])\n",
    "\n",
    "        pitch_counts_df = pitch_counts_df.append(pitch_df)\n",
    "        \n",
    "    # if number of pitches is below thresh, replace count with NaN\n",
    "    def num_pitches_thresh(x):\n",
    "        if x <= thresh:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return x\n",
    "    for col in pitch_counts_df.columns.tolist():\n",
    "        pitch_counts_df[col] = pitch_counts_df[col].apply(num_pitches_thresh)\n",
    "    return pitch_counts_df\n",
    "\n",
    "train_pitch_counts_df = convert_counts_to_features(train_data_categorized, thresh=10)\n",
    "\n",
    "test_pitch_counts_df = convert_counts_to_features(test_data_categorized, thresh=10)\n",
    "\n",
    "print(train_pitch_counts_df.shape)\n",
    "print(train_pitch_counts_df.head())\n",
    "\n",
    "print(test_pitch_counts_df.shape)\n",
    "print(test_pitch_counts_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count the number of used clusters for each observation\n",
    "\n",
    "For comparison to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pitch_counts_df['num_clusters'] = 0\n",
    "\n",
    "for i in range(len(train_pitch_counts_df)):\n",
    "    \n",
    "    train_pitch_counts_df['num_clusters'].iloc[i] = pd.notnull(train_pitch_counts_df.iloc[i,0]) + pd.notnull(train_pitch_counts_df.iloc[i,1]) +\\\n",
    "                                                    pd.notnull(train_pitch_counts_df.iloc[i,2]) + pd.notnull(train_pitch_counts_df.iloc[i,3]) +\\\n",
    "                                                    pd.notnull(train_pitch_counts_df.iloc[i,4]) + pd.notnull(train_pitch_counts_df.iloc[i,5]) +\\\n",
    "                                                    pd.notnull(train_pitch_counts_df.iloc[i,6]) + pd.notnull(train_pitch_counts_df.iloc[i,7]) +\\\n",
    "                                                    pd.notnull(train_pitch_counts_df.iloc[i,8])\n",
    "\n",
    "\n",
    "train_pitch_counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = dict(Counter(train_pitch_counts_df['num_clusters']))\n",
    "plt.bar(list(cluster_count.keys()), list(cluster_count.values()), alpha=0.5)\n",
    "plt.bar([3, 4, 5, 6, 7, 8 , 9], [105, 287, 235, 114, 79, 42, 32], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge K rate data with pitch counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.set_index('obs_index', inplace=True)\n",
    "\n",
    "train_data = train_data[['K/9', 'strike_pct']]\n",
    "\n",
    "train_data = pd.merge(train_data, train_pitch_counts_df, left_index=True, right_index=True)\n",
    "#train_data.drop('num_clusters', axis=1, inplace=True)\n",
    "train_data.fillna(value=0.0, inplace=True)\n",
    "\n",
    "train_data.drop_duplicates(inplace=True)\n",
    "\n",
    "print(f\"Shape of the training data: {train_data.shape}\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.set_index('obs_index', inplace=True)\n",
    "\n",
    "test_data = test_data[['K/9', 'strike_pct']]\n",
    "\n",
    "test_data = pd.merge(test_data, test_pitch_counts_df, left_index=True, right_index=True)\n",
    "#test_data.drop('num_clusters', axis=1, inplace=True)\n",
    "test_data.fillna(value=0.0, inplace=True)\n",
    "\n",
    "test_data.drop_duplicates(inplace=True)\n",
    "\n",
    "print(f\"Shape of the test data: {test_data.shape}\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('K/9', axis=1)\n",
    "y_train = train_data['K/9']\n",
    "\n",
    "X_test = test_data.drop('K/9', axis=1)\n",
    "y_test = test_data['K/9']\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MAPE to evaluate model (is this what the paper usese?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "#    y_true, y_pred = check_arrays(y_true, y_pred)\n",
    "\n",
    "    ## Note: does not handle mix 1d representation\n",
    "    #if _is_1d(y_true): \n",
    "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mean_absolute_percentage_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Off-the-shelf) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor()\n",
    "\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_reg.predict(X_test)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mean_absolute_percentage_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Off-the-shelf) AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_reg = AdaBoostRegressor()\n",
    "\n",
    "ab_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ab_reg.predict(X_test)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mean_absolute_percentage_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized Random Forest\n",
    "\n",
    "Using Scikit-Learn's GridSearchCV to find the optimal set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mean_absolute_percentage_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try scaling features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_0 = MinMaxScaler()\n",
    "X_train[0] = scaler_0.fit_transform(X_train[0].as_matrix().reshape(-1, 1))\n",
    "\n",
    "scaler_1 = MinMaxScaler()\n",
    "X_train[1] = scaler_1.fit_transform(X_train[1].as_matrix().reshape(-1, 1))\n",
    "\n",
    "scaler_2 = MinMaxScaler()\n",
    "X_train[2] = scaler_2.fit_transform(X_train[2].as_matrix().reshape(-1, 1))\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
